{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweetの本文:\n",
      "パナソニックさんが公開した「ChatGPT時代に企業はAIとどう向き合うべきか」の資料が有益なので共有します。AIの導入目的、導入後の数値的な変化、具体的な活用事例とかなり勉強になる内容になっています。\n",
      "\n",
      "こちらです\n",
      "https://meti.go.jp/shingikai/mono_info_service/digital_jinzai/pdf/010_04_00.pdf…\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def get_tweet_content(tweet_url):\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--no-sandbox')\n",
    "\n",
    "    # ChromeDriverのパスを指定\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    \n",
    "    # URLにアクセス\n",
    "    driver.get(tweet_url)\n",
    "\n",
    "    # articleタグが読み込まれるまで待機（最大15秒）\n",
    "    WebDriverWait(driver, 15).until(EC.visibility_of_element_located((By.TAG_NAME, 'article')))\n",
    "\n",
    "    # ページのソースを取得して解析\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    \n",
    "    # Tweet本文を取得\n",
    "    tweet_text = \"\"\n",
    "    tweet_container = soup.find(\"div\", {\"data-testid\": \"tweetText\"})\n",
    "    if tweet_container:\n",
    "        tweet_text = tweet_container.get_text()\n",
    "\n",
    "    driver.quit()\n",
    "    \n",
    "    return tweet_text\n",
    "\n",
    "tweet_url = 'https://x.com/kiyo_innovatia/status/1792366458091315701'\n",
    "# tweet_url = 'https://juu7g.hatenablog.com/entry/Python/blog/notion-api#Block-object-%E3%82%92%E4%BB%8B%E3%81%99%E3%82%8B%E6%93%8D%E4%BD%9C'\n",
    "tweet_content = get_tweet_content(tweet_url)\n",
    "print(\"Tweetの本文:\")\n",
    "print(tweet_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweetの本文:\n",
      "パナソニックさんが公開した「ChatGPT時代に企業はAIとどう向き合うべきか」の資料が有益なので共有します。AIの導入目的、導入後の数値的な変化、具体的な活用事例とかなり勉強になる内容になっています。\n",
      "\n",
      "こちらです\n",
      "https://meti.go.jp/shingikai/mono_info_service/digital_jinzai/pdf/010_04_00.pdf…\n",
      "\n",
      "Tweet内のリンク:\n",
      "https://t.co/ZpuKKdK0ik\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def get_tweet_content_and_links(tweet_url):\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--no-sandbox')\n",
    "\n",
    "    # ChromeDriverのパスを指定\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    \n",
    "    # URLにアクセス\n",
    "    driver.get(tweet_url)\n",
    "\n",
    "    # articleタグが読み込まれるまで待機（最大15秒）\n",
    "    WebDriverWait(driver, 15).until(EC.visibility_of_element_located((By.TAG_NAME, 'article')))\n",
    "\n",
    "    # ページのソースを取得して解析\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    \n",
    "    # Tweet本文とリンクを取得\n",
    "    tweet_text = \"\"\n",
    "    tweet_links = []\n",
    "    tweet_container = soup.find(\"div\", {\"data-testid\": \"tweetText\"})\n",
    "    if tweet_container:\n",
    "        tweet_text = tweet_container.get_text()\n",
    "        links = tweet_container.find_all(\"a\")\n",
    "        for link in links:\n",
    "            tweet_links.append(link['href'])\n",
    "\n",
    "    driver.quit()\n",
    "    \n",
    "    return tweet_text, tweet_links, page_source\n",
    "\n",
    "tweet_url = 'https://x.com/kiyo_innovatia/status/1792366458091315701'\n",
    "tweet_content, tweet_links, page_source = get_tweet_content_and_links(tweet_url)\n",
    "print(\"Tweetの本文:\")\n",
    "print(tweet_content)\n",
    "print(\"\\nTweet内のリンク:\")\n",
    "for link in tweet_links:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timeタグが見つかりませんでした。\n",
      "timeタグが見つかりませんでした。\n",
      "datetime attribute: 2024-05-20 10:27:18+09:00\n"
     ]
    }
   ],
   "source": [
    "import datetime, pytz\n",
    "soup = BeautifulSoup(page_source, 'html.parser')\n",
    "datetime_as = soup.findAll('a', attrs={'aria-label': True, 'role': 'link'})\n",
    "\n",
    "# 日時の抽出\n",
    "for datetime_a in datetime_as:\n",
    "    if datetime_a:\n",
    "        aria_label_datetime = datetime_a.get('aria-label')\n",
    "        time_tag = datetime_a.find('time')\n",
    "        if time_tag:\n",
    "            time_tag_datetime = time_tag.get('datetime')\n",
    "            # print(f\"aria-label: {aria_label_datetime}\")\n",
    "            dt_utc = datetime.datetime.fromisoformat(time_tag_datetime.replace(\"Z\", \"+00:00\"))\n",
    "            dt_utc = dt_utc.replace(tzinfo=pytz.utc)\n",
    "            jst = pytz.timezone('Asia/Tokyo')\n",
    "            dt_jst = dt_utc.astimezone(jst)\n",
    "            print(f\"datetime attribute: {dt_jst}\")\n",
    "        else:\n",
    "            print(\"timeタグが見つかりませんでした。\")\n",
    "    else:\n",
    "        print(\"日時情報が見つかりませんでした。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://help.twitter.com/using-twitter/twitter-supported-browsers\n",
      "https://twitter.com/tos\n",
      "https://twitter.com/privacy\n",
      "https://support.twitter.com/articles/20170514\n",
      "https://legal.twitter.com/imprint.html\n",
      "https://business.twitter.com/en/help/troubleshooting/how-twitter-ads-work.html?ref=web-twc-ao-gbl-adsinfo&utm_source=twc&utm_medium=web&utm_campaign=ao&utm_content=adsinfo\n",
      "/login\n",
      "/i/flow/signup\n",
      "https://x.com/en/privacy\n",
      "/\n",
      "/settings\n",
      "/kiyo_innovatia\n",
      "/kiyo_innovatia\n",
      "/kiyo_innovatia\n",
      "https://t.co/ZpuKKdK0ik\n",
      "/kiyo_innovatia/status/1792366458091315701/photo/1\n",
      "/kiyo_innovatia/status/1792366458091315701/photo/2\n",
      "/kiyo_innovatia/status/1792366458091315701/photo/3\n",
      "/kiyo_innovatia/status/1792366458091315701/photo/4\n",
      "/kiyo_innovatia/status/1792366458091315701\n",
      "/kiyo_innovatia/status/1792366458091315701/retweets\n",
      "/kiyo_innovatia/status/1792366458091315701/retweets/with_comments\n",
      "/kiyo_innovatia/status/1792366458091315701/likes\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(page_source, 'html.parser')\n",
    "links = soup.find_all(\"a\")\n",
    "for link in links:\n",
    "    print(link['href'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
